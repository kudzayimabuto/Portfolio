{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#pip install pyspark\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "import os\n",
    "import gc\n",
    "from multiprocessing import Pool, cpu_count\n",
    "p = Pool(cpu_count())\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as Date\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "spark = SparkSession.builder.appName('play').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_validation = reduce_mem_usage(pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv'))\n",
    "calendar = reduce_mem_usage(pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv'))\n",
    "sell_prices = reduce_mem_usage(pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_prices['id'] = sell_prices.item_id+'_'+ sell_prices.store_id+'_validation'\n",
    "ex_columns = ['item_id','dept_id','cat_id','store_id','state_id']\n",
    "sales_train_validation = reduce_mem_usage(sales_train_validation.drop(ex_columns, axis = 1))\n",
    "sales_train = reduce_mem_usage(sales_train_validation.melt(id_vars=[\"id\"], \n",
    "        var_name=\"d\", \n",
    "        value_name=\"sales_units\"))\n",
    "\n",
    "\n",
    "day_d = reduce_mem_usage(calendar[['date','d','wm_yr_wk']])\n",
    "sales_date = reduce_mem_usage(sales_train.merge(day_d, on = 'd', how = 'left'))\n",
    "sell_prices = reduce_mem_usage(sell_prices[['id','sell_price','wm_yr_wk']])\n",
    "\n",
    "df_final = reduce_mem_usage(sales_date.merge(sell_prices, on=['id','wm_yr_wk'], how = 'left'))\n",
    "df_final['y'] = df_final['sales_units']\n",
    "\n",
    "df_final = df_final[df_final['y']>=1]\n",
    "x_trans, lamb = boxcox(df_final['y'])\n",
    "df_final['y'] = x_trans\n",
    "\n",
    "#create holidays data frame\n",
    "\n",
    "event_name = calendar[['event_name_1','date']].dropna(axis = 0)\n",
    "event_name.columns = ['holiday','ds']\n",
    "event_name['lower_window'] = 0\n",
    "event_name['upper_window'] = 1\n",
    "#reduce dataframe size\n",
    "df_final = reduce_mem_usage(df_final)\n",
    "event_name = reduce_mem_usage(event_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_date=pd.DataFrame()\n",
    "calendar=pd.DataFrame()\n",
    "sell_prices=pd.DataFrame()\n",
    "sales_train_validation=pd.DataFrame()\n",
    "sales_train=pd.DataFrame()\n",
    "day_d=pd.DataFrame()\n",
    "sales_date=pd.DataFrame()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "filter_df = (df_final['id'].value_counts()).reset_index()\n",
    "filter_df.columns = ['id', 'id_count']\n",
    "filter_df['0.001'] = filter_df['id_count'].between(0,19)\n",
    "filter_df['0.005'] = filter_df['id_count'].between(20,29)\n",
    "filter_df['0.01'] = filter_df['id_count'].between(30,59)\n",
    "filter_df['0.2'] = filter_df['id_count'].between(60,89)\n",
    "filter_df['0.25'] = filter_df['id_count'].between(90,119)\n",
    "filter_df['0.3'] = filter_df['id_count']>=120\n",
    "filter_df[['0.001','0.005','0.01','0.2','0.25','0.3']]=filter_df[['0.001','0.005','0.01','0.2','0.25','0.3']]*1\n",
    "filter_df = reduce_mem_usage(filter_df.melt(['id','id_count'], var_name='change_point').query('value == 1').sort_values(['id', 'change_point']).drop('value',1))\n",
    "filter_df['weekly'] = filter_df['id_count']>=10\n",
    "filter_df['yearly'] = filter_df['id_count']>=365\n",
    "filter_df =filter_df.head(100)\n",
    "#create F1 to F29 days format for submission\n",
    "day = range(1, 29,1)\n",
    "df_day = pd.DataFrame(day)\n",
    "df_day.columns = ['day']\n",
    "df_day['day'] = 'F'+df_day['day'].astype(str)\n",
    "result_day = df_day['day'].tolist()\n",
    "\n",
    "#Merge filter_df and final df\n",
    "historic_data = df_final.merge(filter_df, on = 'id', how = 'inner')\n",
    "#clear dataframes from memory\n",
    "filter_df=pd.DataFrame()\n",
    "df_final=pd.DataFrame()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data['ds']= historic_data['date']\n",
    "historic_data = historic_data.drop(['date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema =StructType([\n",
    "  StructField('id',StringType()),\n",
    "  StructField('F1', FloatType()),\n",
    "  StructField('F2', FloatType()),\n",
    "  StructField('F3', FloatType()),\n",
    "  StructField('F4', FloatType()),\n",
    "  StructField('F5', FloatType()),\n",
    "  StructField('F6', FloatType()),\n",
    "  StructField('F7', FloatType()),\n",
    "  StructField('F8', FloatType()),\n",
    "  StructField('F9', FloatType()),\n",
    "  StructField('F10', FloatType()),\n",
    "  StructField('F11', FloatType()),\n",
    "  StructField('F12', FloatType()),\n",
    "  StructField('F13', FloatType()),\n",
    "  StructField('F14', FloatType()),\n",
    "  StructField('F15', FloatType()),\n",
    "  StructField('F16', FloatType()),\n",
    "  StructField('F17', FloatType()),\n",
    "  StructField('F18', FloatType()), \n",
    "  StructField('F19', FloatType()), \n",
    "  StructField('F20', FloatType()),\n",
    "  StructField('F21', FloatType()), \n",
    "  StructField('F22', FloatType()), \n",
    "  StructField('F23', FloatType()),\n",
    "  StructField('F24', FloatType()),\n",
    "  StructField('F25', FloatType()),\n",
    "  StructField('F26', FloatType()),\n",
    "  StructField('F27', FloatType()),\n",
    "  StructField('F28', FloatType())\n",
    "  ])\n",
    "\n",
    "test_schema =StructType([\n",
    "  StructField('id',StringType()),\n",
    "  StructField('ds',DateType()),\n",
    "  StructField('yhat', FloatType())\n",
    "])\n",
    "history_spark = spark.createDataFrame(historic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(test_schema, PandasUDFType.GROUPED_MAP)\n",
    "def profesy(history_spark):\n",
    "\n",
    "    #history_spark['ds']= history_spark['date']\n",
    "    #history_spark = history_spark.drop(['date'], axis = 1)\n",
    "    #history_spark['ds'] = pd.to_datetime(history_spark['ds'])\n",
    "\n",
    "    #model = Prophet(changepoint_prior_scale=float(history_spark['change_point'].max()), \n",
    "    #            daily_seasonality=False, \n",
    "    #            weekly_seasonality=history_spark['weekly'].max(),\n",
    "    #            yearly_seasonality=history_spark['yearly'].max()\n",
    "    #            #holidays = event_name\n",
    "    #               )\n",
    "    model = Prophet()\n",
    "    model.fit(history_spark)\n",
    "    build_forecast = model.make_future_dataframe(periods=28,freq='D',include_history=False)\n",
    "    forecast = model.predict(build_forecast)\n",
    "    \n",
    "    df_forecast = forecast[[\"ds\", \"yhat\"]]\n",
    "    #forecast = pd.DataFrame()\n",
    "    df_forecast['id'] = history_spark['id'].max()\n",
    "    #df_forecast[\"yhat\"] = inv_boxcox(df_forecast[\"yhat\"], lamb)\n",
    "    #df_forecast[\"yhat\"] = df_forecast[\"yhat\"].round()\n",
    "    #df_forecast['fday'] = result_day\n",
    "    #df_forecast = df_forecast.drop(['ds'], axis = 1)\n",
    "    #df_forecast = df_forecast.pivot_table(values ='yhat',index='id', columns='fday').reset_index()\n",
    "    \n",
    "    return df_forecast\n",
    "#df_forecast[['id','F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10','F11','F12','F13','F14','F15','F16','F17','F18','F19','F20','F21','F22','F23','F24','F25','F26','F27','F28']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = history_spark.groupBy('id').apply(profesy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df[['yhat', 'yhat_lower', 'yhat_upper']] = inv_boxcox(results_df[['yhat', 'yhat_lower', 'yhat_upper']], lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
